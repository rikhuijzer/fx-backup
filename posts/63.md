---
created: '2025-05-08 07:30:52 UTC'
updated: '2025-05-08 07:53:53 UTC'
---

# AI learning rate and some thoughts

Now that DeepSeek released their new model, I'm thinking again about where AI is heading.
For some technologies such as electric cars or batteries, I think I have a reasonable idea of where they are heading thanks to [learning curves](https://ourworldindata.org/learning-curve).
Due to price decreases, we will probably see more electric cars, drones, [ships](https://www.sustainable-ships.org/stories/2021/worlds-first-electric-cargo), and [trucks](https://www.youtube.com/@electrictrucker).
Oh, and of course stoves that can [boil water in 40 seconds](https://youtu.be/YdawGen0QPc).
For AI, I'm not sure.
In this blog post, I'll try to estimate the learning curve for AI and see if I can use that to make predictions.

Learning curves, also known as _Wright's Law_, describe the phenomenon that for some technologies the cost decreases as the number of units produced increases.
One of the most famous examples is the cost of computer chips also known as Moore's Law.
Although Moore's Law is often associated with the number of transistors on a chip, it is actually about cost as well.
From the [paper](https://cse.buffalo.edu/~bina/amrita/summer2017/CSE541/Day1/moorePaper.pdf) by Gordon E. Moore:

> "In 1970, the manufacturing cost per component can be expected to be only a tenth of the present cost."

And Moore was right.
In 1961, the price per floating point operation was about \$190 billion in inflation-adjusted dollars.
In 2023, the price for a floating point operation was about \$0.0125.
That means that the price went down by about 13 orders of magnitude.

Furthermore, while the price went down, the quality went up.
Modern processors are less likely to break down and are more energy efficient.

Another recent example is solar panels.
This is what happened to the price of solar panels:

![Solar PV prices versus cumulative capacity](/files/099b88e34240723a)

From 2000 to 2023, the price of solar panels has decreased from about 7 \$/W to 0.3 \$/W.
To some, this might sound like a small difference compared to the step from 100 \$/W to 10 \$/W, but those absolute numbers are not the best way to compare the two.
Instead, think about how much panels you can buy for a real-world use-case.
For example, a house typically has about 8 kW of solar panels.
In 2000 this would cost about 7 \$/W * 8 kW = \$56,000.
In 2023, the same amount of panels would cost only 0.3 \$/W * 8 kW = \$2,400.

While the price of solar panels is going down exponentially, the amount of solar installations is going up exponentially:

![Installed solar PV capacity](/files/8e329d182f464a95)

In [another blog post](/posts/battery-learning-curves), I showed that Tony Seba has correctly predicted that batteries would roughly become 14% cheaper every year:

![Battery cost decline](/files/5163d85a393d8bb8)

In this graph, the lines show 12% and 16% cost decline as was predicted by Tony Seba in 2010 and 2014.
As you can see, the actual cost decline has indeed been very close to Tony's predictions.

So maybe we can use the same method to estimate the future of AI?

## Learning rates

Before going into AI, let's first look at learning rates for other technologies.
For solar panels for example, we know that the cost decline occured over $2023 - 1975 = 48$ years.
The rate can then be calculated as follows:
$(0.3 / 100)^{1/48} = 0.886$.
Just to verify, $100 * 0.886^{48} â‰ˆ 0.3$.

This means that the costs declines by roughly $1-0.886=12\%$ every year.
So, the _learning rate_ for solar panels is 12%.

Using the same method, I estimated the following numbers with the use of Fermi estimates (also known as _order-of-magnitude_ estimates).
The idea of Fermi estimates is that over- and underestimates can cancel each other out when multiplying terms.
Next to the cost declines, I also estimated the production growth.

Technology | Cost decline per year | Production growth per year
:-- | --: | --:
Solar PV | 12% | 26%[^solarprod]
Batteries | 14%[^batterycost] | 50%[^batteryprod]
Data storage | 35%[^datastoragecost] | 5%[^datastorageprod]
Network transport | 60%[^networkcost] | 50%[^networkprod]
Computer chips | 40%[^chipcost] | 6%[^chipprod]

In most cases, the cost went down while the quality went up.
For example, modern batteries have much longer lifetimes (and warranties) than older batteries.
Also, modern SSDs are much faster than older hard disks.

I'll now speculate a bit on why the production growth is so different for the different technologies.
My guess is that the production growth is largely dependent on how much better and cheaper the technology is compared to the alternative.
When technologies replace something else, the change goes very rapidly.
But at some point, they will reach a point where they have replaced all alternatives and then growth will slow down.

Data storage and computer chips have already replaced all alternatives.
In 1953, you could choose between a hard disk drive or magnetic tape.
The hard disk drive at some point became cheaper and better than the magnetic tape leading everyone to switch.
Now, there is not competition.
Most places already use SSDs, so growth is limited to finding new use-cases.

However, batteries are replacing existing use-cases.
The benefit batteries have in cars is that they require less maintenance and are about 2-3 times more energy efficient than combustion cars[^electriccars].
It's similar for network transport.
It looks like things are still moving over from physical transport to network transport.
More and more meetings are held online instead of in person.
Even shopping of course is partially replaced by moving pictures of the product over the internet instead of physically going to the store.

## AI learning rate

AI is what caused me to write this blog post.
I already knew that the price per teraflop of compute power has been going down very rapidly.
For example, Bill Dally from Nvidia showed that inference performance per chip has [increased by 1000x in the last 10 years](https://www.youtube.com/watch?v=kLiwvnr4L80&t=664s).
This means a performance increase of about 100% per year while the price per chip has not increased by 1000x.

On top of this, Deepseek released their new model and people have estimated that training cost has decreased from about \$500 million to about \$5 million.
Similarly, their inference pricing is also an order of magnitude lower than OpenAI.
It looks like there are two factors that are rapidly reducing the cost of AI.
The hardware costs are going down while algorithms are also becoming more efficient.

I was curious how fast the cost of AI is going down.
For this, let's use the price of inference:

Date | Model | Price per 1M output tokens
--: | :-- | --:
Feb 2023 | OpenAI Davinci | \$20[^davinci]
Mar 2023 | OpenAI GP-4 | \$60[^gpt4]
Nov 2023 | OpenAI GPT-4 Turbo | \$30[^gpt4]
May 2024 | OpenAI GPT-4o | \$15[^gpt4]
Jul 2024 | OpenAI GPT-4o Mini | \$0.6[^gpt4]
Jan 2025 | Llama 3.3 Instruct 70B | \$0.7[^artificialanalysis]
Jan 2025 | DeepSeek V3 | \$1.1[^artificialanalysis]

This means the cost decline per year was 77%[^annualrate].
Also here, the price went down while the quality/performance went up.
What the cost decline means is that inference pricing for state-of-the-art models will probably go down to \$0.24 per 1M output tokens around Jan 2026, \$0.05 per Jan 2027, and \$0.01 per Jan 2028.
If you keep quality constant, the decrease is [about 1000% per year](https://darioamodei.com/on-deepseek-and-export-controls).

As a practical example of what this decrease for state-of-the-art models means, let's say you use an AI copilot in your text editor as a autocomplete or you use AI to generate to starting point for text documents.
Then, you would probably generate about 1000 tokens per day[^tokens].
But say 3000 tokens per day to be on the safe side and to also account for input tokens.

Then the monthly cost is currently around \$0.03.
Given that it probably saves me about 30 * 30 = 900 minutes per month, this is multiple orders of magnitude cheaper than the thing it replaces, namely me.

Now to question is where this is heading.
Based on the reasoning above, I have to re-conclude that AI-powered applications will replace many existing applications.
A lot of people currently are trying to use AI, but have a hard time finding use-cases.
Based on current stock prices, many people think that these new applications will come out of the big tech.
I think that's unlikely.

AI in code editors is my favorite example of this.
Microsoft was early with releasing Copilot.
Simply put, Copilot is autocomplete for programmers.
But while Microsoft had everything it needed to release the best AI code editor, they didn't.
Microsoft had the biggest AI infrastructure (Azure), they had the most used code editor (Visual Studio Code), they had the most capable models (via OpenAI), and they were the first to release an AI code editor.
Still, Cursor AI came in and many programmers switched to Cursor.

DeepSeek is another example.
Microsoft, Google, Meta, OpenAI, and Anthropic all had the most experience and the most resources.
Still, DeepSeek came out of the blue with [multiple innovations](https://stratechery.com/2025/deepseek-faq/).
DeepSeek's CEO, Liang Wenfeng, argues that this is because this is a [new technology](https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas).
According to Wenfeng all you need is small teams with "fresh graduates from top universities, PhD candidates in their fourth or fifth year, and some young people who graduated just a few years ago."
My interpretation is that young people are better at finding new solutions to new problems, whereas older and more experienced people are better at improving existing solutions.
This is why improvements in the TSMC process need a PhD and years of industry experience, but advances in Gemini at Google were made by Sholto who was [only in the field for 1.5 years](https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas).

I think that the same will happen with other AI-powered applications.
The big companies will probably try to figure out the best applications, but it is likely that new players will come up with new applications that have better AI integrations.
As DeepSeek's CEO put it, "Using Internet business logic to discuss future AI profit models is like discussing General Electric and Coca-Cola when Pony Ma _[Founder of Tencent; one of the biggest companies in the video game industry]_ was starting his business."
Taleb would probably agree.
In a [recent interview](https://youtu.be/cidH25tVggQ) he said that if you wanted to invest in the internet in 1999, you would probably have invested in Altavista.
But then Google came out of nowhere and displaced Altavista.
Note also that Google was built by a few young and highly technical people.

As a Google employee predicted in 2023, open source has a [good chance of winning](https://semianalysis.com/2023/05/04/google-we-have-no-moat-and-neither/).
Maybe we will see replacements for Word, Photoshop, Slack, or PowerPoint.
Also, and this is more exciting, we will probably see more new/niche applications that are only possible due to AI.

For now, I'll leave it here.
I hoped that estimating the learning rate would give me at least some idea of where AI would be heading, but I feel like I still don't know.
What I do know is that the learning rate is currently very high.
AI is already very cheap and will probably become much cheaper fast.

It's going to be interesting.

[^solarprod]: $(61200/7050)^{(1/(2023-2007))}$ means 12% growth per year based on <https://www.statista.com/statistics/668764/annual-solar-module-manufacturing-globally/>, but Wikipedia says 26% <https://en.wikipedia.org/wiki/Growth_of_photovoltaics> and matches better the growth I have seen in other sources.

[^batterycost]: <https://huijzer.xyz/posts/battery-learning-curves/>.

[^batteryprod]: Taking China as a proxy for global production since China produces at least more than half of the world's batteries. $(950/150)^{(1/(2023-2019))} = 60%$. <https://about.bnef.com/blog/ev-slowdown-countered-by-energy-storage-boom/>. I've put the estimate at 50% since the production growth in the West is about 30% per year and the West produces around 30% of the world's batteries.

[^datastoragecost]: Kryder's Law. <https://youtu.be/Kxryv2XrnqM?t=480>.

[^datastorageprod]: $((350+350)/(450+40))^{(1/(2020-2012))}$. Figure 1 in <http://dx.doi.org/10.2788/89220>.

[^networkcost]: Butter's Law of Photonics. <https://youtu.be/Kxryv2XrnqM?t=480>.

[^networkprod]: $(40/0.16)^{(1/(2020-2006))}$. Figure 1 in <http://dx.doi.org/10.1360/972013-1054>.

[^chipcost]: $(0.0125/190 380 000 000)^{(1/(2023-1961))}$ and $(0.0125/56940)^{(1/(2023-1997))}$ give $1-0.61=39\%$ and $1-0.55=45\%$ respectively. <https://en.wikipedia.org/wiki/Floating_point_operations_per_second>. Tony Seba also estimates about 40% (https://youtu.be/Kxryv2XrnqM?t=469).

[^chipprod]: Capacity increased with 9%, 8%, 5%, 6%, and 7% respectively from 2021 to 2025. <https://www.ept.ca/2024/06/global-semi-fab-capacity-projected-to-expand/>.

[^davinci]: <https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/>.

[^gpt4]: <https://www.nebuly.com/blog/openai-gpt-4-api-pricing>.

[^artificialanalysis]: <https://artificialanalysis.ai/>.

[^annualrate]: $(1/20)^{(1/(2025-2023))}$.

[^tokens]: This is also roughly what I use via Open WebUI with DeepSeek V3.

[^electriccars]: Also when the energy comes from a power plant instead of solar panels. Most Western countries in the least efficient case use gas power plants nowadays, which have an efficiency of about 80%. The electric car including battery has an efficiency of about 90%, so 90% * 80% = 72%. Petrol cars have an efficiency of about 20%. In winter this is a bit better because the engine heat is useful for heating the car.

