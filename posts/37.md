---
created: '2025-05-05 15:25:23 UTC'
updated: '2025-05-05 15:25:23 UTC'
---

# Nested cross-validation

Nested cross-validation is said to be an improvement over cross-validation.
Unfortunately, I found most explanations quite confusing, so decided to simulate some data and see what happens.

In this post, I simulate two models: one linear model which perfectly fits the data and one which overfits the data.
Next, cross-validation and nested cross-validation are plotted.
To keep the post short, I've hidden the code to produce the plots.

```julia
import MLJLinearModels
import MLJDecisionTreeInterface

using DataFrames: DataFrame, select, Not
using Distributions: Normal
using CairoMakie: Axis, Figure, lines, lines!, scatter, scatter!, current_figure, axislegend, help, linkxaxes!, linkyaxes!, xlabel!, density, density!, hidedecorations!, violin!, boxplot!, hidexdecorations!, hideydecorations!
using MLJ: CV, evaluate, models, matching, @load, machine, fit!, predict, predict_mode, rms
using Random: seed!
using Statistics: mean, std, var, median
using MLJTuning: TunedModel, Explicit
using MLJModelInterface: Probabilistic, Deterministic
```

```julia
y_true(x) = 2x + 10;
```

```julia
y_real(x) = y_true(x) + rand(Normal(0, 40));
```

```julia
indexes = 1.0:100;
```

```julia
df = let
    seed!(0)
    DataFrame(x = indexes, y = y_real.(indexes))
end
```

x | y
--- | ---
1.0 | 49.7188
2.0 | 19.3569
3.0 | 77.0028
4.0 | 22.956
5.0 | -28.2309
6.0 | 34.4727
7.0 | 14.6143
8.0 | -17.4941
9.0 | 46.4924
10.0 | 26.7763
... | ...
100.0 | 174.396

