---
created: '2025-05-02 09:40:19 UTC'
updated: '2025-05-02 11:43:28 UTC'
---

# Simple and binary regression

One of the most famous scientific discoveries was Newton's laws of motion.
The laws allowed people to make predictions.
For example, the acceleration for an object can be predicted given the applied force and the mass of the object.
Making predictions remains a popular endeavor.
This post explains the simplest way to predict an outcome for a new value, given a set of points.

To explain the concepts, data on apples and pears is generated.
Underlying relations for the generated data are known.
The known relations can be compared to the results from the regression.

## Generating data
The goal is to predict whether a fruit is an apple or a pear.
Say we have a sample of size $n$.
Say the sample consist of two properties for each fruit, namely

- height, and
- width.

The properties for the fruit at index $i$ are respectively denoted by $h_i$ and $w_i$.
Prediction will be done for the fruit type, which is denoted by $y_i$.
The sample indices can be visualised as follows.

I | H | W | Y
--- | --- | --- | ---
1 | $h_1$ | $w_1$ | $y_1$
2 | $h_2$ | $w_2$ | $y_2$
.. | .. | .. | ..
n | $h_n$ | $w_n$ | $y_n$

Let half of the elements be apples and half of the elements be pears.
So, $n$ is even.
Let

$$
y_{i} =
\begin{cases}
\text{apple} & \text{if $i$ is odd}, \: \text{and} \\
\text{pear} & \text{if $i$ is even}.
\end{cases}
$$

Let the height and fruit type be correlated.
To this end draw elements from respectively $N(10, 1)$ and $N(12, 1)$:

$$
h_{i} =
\begin{cases}
x \sim N(10, 1) & \text{if $Y_i$ is apple}, \: \text{and} \\
x \sim N(12, 1) & \text{if $Y_i$ is pear}.
\end{cases}
$$

Let the height and width also be correlated.
Define the width to be 0.6 times the height.
Specifically,

$$
w_i = 0.6 h_i.
$$

In Julia, this can be defined as

```julia
begin
    using AlgebraOfGraphics
    using CairoMakie
    using DataFrames
    using Distributions: Binomial, Normal
    using GLM: LogitLink, @formula, coef, glm, lm, predict
    using Random: seed!
    using StableRNGs
    using Statistics
end
```

```julia
r_2(x) = round(x; digits=2);
```

```julia
df = let
    rng = StableRNG(42)
    n = 12
    I = 1:n
    Y = [i % 2 != 0 ? "apple" : "pear" for i in I]
    H = r_2.([y == "apple" ? rand(rng, Normal(10, 1)) : rand(rng, Normal(12, 1)) for y in Y])
    W = r_2.([0.7h for h in H])

    DataFrame(; I, H, W, Y)
end
```

| I | H | W | Y |
|----------|----------|----------|----------|
| 1        | 9.33    | 6.53     | "apple"  |
| 2        | 12.45    | 8.71     | "pear"   |
| 3        | 11.37    | 7.96     | "apple"  |
| 4        | 13.31    | 9.32     | "pear"   |
| 5        | 10.13    | 7.09     | "apple"  |
| 6        | 12.68    | 8.88     | "pear"   |
| 7        | 8.98     | 6.29     | "apple"  |
| 8        | 11.21    | 7.85     | "pear"   |
| 9        | 11.77    | 8.24     | "apple"  |
| 10       | 13.3     | 9.31     | "pear"   |
| 11       | 8.36     | 5.85     | "apple"  |
| 12       | 12.79    | 8.95     | "pear"   |

## Simple linear regression

A *simple linear regression* fits a line through points in two dimensions.
It should be able to infer the relation between $H$ and $W$.

![w-h](/files/030da1d7fd3cf8e3)

The algorithmic way to fit a line is via the *method of least squares*.
Any straight line can be described by a linear equation of the form $y = p_1 x + p_0$, where the first parameter $p_0$ is the intercept with $y$ and the second parameter $p_1$ is the slope.
Adding an error $e$, and rewriting gives

$$
\begin{aligned}
y_i & = p_0 + p_1 x_i + e_i \\
e_i & = y_i - p_0 - p_1 x_i. \\
\end{aligned}
$$

An algorithm could now be defined to naively minimize the sum of all the errors

$$
S'(p_0, p_1) = \sum_{i=1}^n e_i,
$$

with respect to the choice of $p_0$ and $p_1$.
This would not always result in a well fitted line because errors might cancel each other out.
For example, when $e_1 = 10$ and $e_2 = -10$, then $e_1 + e_2 = 0$.
This is solved by squaring the errors.
The method of least squares minimizes

$$
S_l(p_0, p_1) = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - p_0 - p_1 x_i)^2
$$

with respect to the choice of $p_0$ and $p_1$ (Rice, 2006).
The simplest estimator for the points is the mean.
We can plot this and show horizontal lines for the errors.

![w-h-errors](/files/e6a2cc1c27d61a37)

We can generalize the sum of squares error calculation to

$$
S(U, V) = \sum_{i=1}^n (u_i - v_i)^2,
$$

for arrays $U$ and $V$.

```julia
S(U, V) = sum((U .- V).^2)
```

Then the squared sum of the errors for this simplest estimator is

```julia
r_2(S(df.H, repeat([mean(df.H)], length(df.H))))
```

```
33.0
```

This error cannot be compared to other errors, since it is not standardized.
A standardized metric would be the Pearson correlation coefficient $r$.
See the blog post on [correlations](/posts/correlations) for more information.

```julia
m1 = lm(@formula(H ~ W), df);
```

![w-h-fitted](/files/6a34c762a2aa7881)

```julia
"""
    predict_value(model, x)

This is just a convenience function around `GLM.predict`.
"""
function predict_value(model, x)
    return first(skipmissing(predict(model, DataFrame(W = [x]))))
end;
```

The intercept and slope for the fitted line are

```julia
intercept(linear_model) = coef(linear_model)[1]
```

```julia
slope(linear_model) = coef(linear_model)[2]
```

```julia
r_2(intercept(m1))
```

```
0.0
```

```julia
r_2(slope(m1))
```

```
1.43
```

## Binary logistic regression

Next, lets try to estimate the relation between $Y$ and $W$.
The method of least squares is unable to calculate an error for "apple" and "pear".
A workaround is to encode "apple" as 0 and "pear" as 1.
A line can now be fitted.

```julia
df_digits = let
    digits = [i % 2 != 0 ? 0 : 1 for i in df.I]
    df_digits = DataFrame(df)
    df_digits[!, :Y_digit] = digits
    df_digits
end;
```

```julia
m2 = lm(@formula(Y_digit ~ W), df_digits);
```

![w-y-digit](/files/bbdb4e4c198d10b0)

As can be observed, the model does not take into account that $Y$ is a binary variable.
The model even predicts values outside the expected range, that is, values outside the range 0 to 1.
A better fit is the logistic function.

```julia
m3 = glm(@formula(Y_digit ~ W), df_digits, Binomial(), LogitLink());
```

![w-y-digit logit](/files/8810668acd64c5b1)

The correlation coefficient $r$ should not be used to compare the models, since the logistic model only predicts the class.
In other words, the logistic model is a classifier.
Classification *accuracy* is a better metric:

$$
\text{accuracy} = \frac{\text{number of correct predictions}}{\text{total number of predictions}}.
$$

```julia
accuracy(trues, pred) = count(trues .== pred) / length(pred)
```

The threshold is set to 0.5 to get a binary prediction from both models.
More precisely: let $\text{pred}(x_i)$ denote the prediction for $y_i$, and $y_i'$ denote the binary prediction for $y_i$.
For each prediction

$$
y_i' =
\begin{cases}
1 & \text{if $0.5 < \text{pred}(x_i)$}, \: \text{and} \\
0 & \text{if $\text{pred}(x_i) \leq 0.5$}. \\
\end{cases}
$$

```julia
binary_values(model) = [0.5 < x ? 1 : 0 for x in predict(model)]
```

```julia
let
    model = ["Linear regression", "Logistic regression"]
    digits = df_digits.Y_digit
    error = r_2.([S(digits, predict(m2)), S(digits, predict(m3))])
    accuracy = r_2.([accuracy(digits, binary_values(m2)), accuracy(digits, binary_values(m3))])

    DataFrame(; model, error, accuracy)
end
```

Model | Error | Accuracy
--- | --- | ---
Linear regression | 1.11 | 0.75
Logistic regression | 1.04 | 0.83

As can be seen, the error is lower for the logistic regression.
However, for the accuracy both models score the same in this case.
This is due to the fact that there is only a very small area where the linear and logistic model make different predictions.
When this area becomes bigger (for example, when doing regression in multiple dimensions) or when more points lie in this area, then the accuracy for the logistic regression will be better compared to the linear regression.

## References

Rice, J. A. (2006).
Mathematical statistics and data analysis.
Cengage Learning.

## Appendix

Built with Julia 1.11.5 and

AlgebraOfGraphics 0.8.13 \
CairoMakie 0.12.16 \
DataFrames 1.7.0 \
Distributions 0.25.113 \
GLM 1.9.0 \
Random 1.11.0 \
StableRNGs 1.0.2 \
Statistics 1.11.1

