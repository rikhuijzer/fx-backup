---
created: '2025-05-02 09:40:19 UTC'
updated: '2025-05-02 09:41:35 UTC'
---

# Simple and binary regression

One of the most famous scientific discoveries was Newton's laws of motion.
The laws allowed people to make predictions.
For example, the acceleration for an object can be predicted given the applied force and the mass of the object.
Making predictions remains a popular endeavor.
This post explains the simplest way to predict an outcome for a new value, given a set of points.

To explain the concepts, data on apples and pears is generated.
Underlying relations for the generated data are known.
The known relations can be compared to the results from the regression.

## Generating data
The goal is to predict whether a fruit is an apple or a pear.
Say we have a sample of size $n$.
Say the sample consist of two properties for each fruit, namely

- height, and
- width.

The properties for the fruit at index $i$ are respectively denoted by $h_i$ and $w_i$.
Prediction will be done for the fruit type, which is denoted by $y_i$.
The sample indices can be visualised as follows.

I | H | W | Y
--- | --- | --- | ---
1 | $h_1$ | $w_1$ | $y_1$
2 | $h_2$ | $w_2$ | $y_2$
.. | .. | .. | ..
n | $h_n$ | $w_n$ | $y_n$

Let half of the elements be apples and half of the elements be pears.
So, $n$ is even.
Let

```math
y_{i} =
\begin{cases}
\text{apple} & \text{if $i$ is odd}, \: \text{and} \\
\text{pear} & \text{if $i$ is even}.
\end{cases}
```

Let the height and fruit type be correlated.
To this end define

```math
h_{i} =
\begin{cases}
\text{one sample of size 1 drawn from } N(10, 1) & \text{if $Y_i$ is apple}, \: \text{and} \\
\text{one sample of size 1 drawn from } N(12, 1) & \text{if $Y_i$ is pear}.
\end{cases}
```

Let the height and width also be correlated.
Define the width to be 0.6 times the height.
Specifically,

$$w_i = 0.6 h_i.$$

In Julia, this can be defined as

```julia
begin
    using AlgebraOfGraphics
    using CairoMakie
    using DataFrames
    using Distributions: Binomial, Normal
    using GLM: LogitLink, @formula, coef, glm, lm, predict
    using Random: seed!
    using StableRNGs
    using Statistics
end
```

```julia
r_2(x) = round(x; digits=2);
```

```julia
df = let
    rng = StableRNG(42)
    n = 12
    I = 1:n
    Y = [i % 2 != 0 ? "apple" : "pear" for i in I]
    H = r_2.([y == "apple" ? rand(rng, Normal(10, 1)) : rand(rng, Normal(12, 1)) for y in Y])
    W = r_2.([0.7h for h in H])

    DataFrame(; I, H, W, Y)
end
```

| I | H | W | Y |
|----------|----------|----------|----------|
| 1        | 9.33    | 6.53     | "apple"  |
| 2        | 12.45    | 8.71     | "pear"   |
| 3        | 11.37    | 7.96     | "apple"  |
| 4        | 13.31    | 9.32     | "pear"   |
| 5        | 10.13    | 7.09     | "apple"  |
| 6        | 12.68    | 8.88     | "pear"   |
| 7        | 8.98     | 6.29     | "apple"  |
| 8        | 11.21    | 7.85     | "pear"   |
| 9        | 11.77    | 8.24     | "apple"  |
| 10       | 13.3     | 9.31     | "pear"   |
| 11       | 8.36     | 5.85     | "apple"  |
| 12       | 12.79    | 8.95     | "pear"   |

## Simple linear regression

A *simple linear regression* fits a line through points in two dimensions.
It should be able to infer the relation between $H$ and $W$.

![w-h](/files/030da1d7fd3cf8e3)

