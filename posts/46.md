---
created: '2025-05-07 17:01:24 UTC'
updated: '2025-05-07 17:08:37 UTC'
---

# Bayesian Latent Profile Analysis (mixture modeling)

_Updated on 2021-12-15: Include ordered constraint._

This post discusses some latent analysis techniques and runs a Bayesian analysis for example data where the outcome is continuous, also known as _latent profile analysis_ (LPA).
My aim will be to clearly visualize the analysis so that it can easily be adjusted to different contexts.

In essence, latent analyses about finding hidden groups in data (Oberski, 2016).
Specifically, they are called _mixture models_ because the underlying distributions are mixed together.

For example, suppose we had data about dog weights and did not register the breed.
If we assume that the sample only consists of Australian Sheperds and American Terriers, that American Terriers are larger on average, and that both groups are normally distributed, then we can take the observed data and estimate the latent distributions.
For example, we can generate some data:

```julia
begin
    using Bijectors: OrderedBijector, ordered, inv
    using CairoMakie
    using DataFrames
    using StableRNGs: StableRNG
    using Statistics: mean, median, std
    using Turing
end
```

```julia
d1 = Normal(22, 2.4); # Australian Shepherd.
```

```julia
d2 = Normal(28, 2.8); # American Terrier.
```

```julia
combined = let
    rng = StableRNG(1)
    [i % 2 == 1 ? rand(rng, d1) : rand(rng, d2) for i in 1:200]
end;
```

and visualize it:

![Combined Sheperd and Terrier data](/files/fbee6bfbb059f254)

Next to LPA, this problem is also known as _gaussian (finite) mixture modelling_.
When the observed variables are discrete, the appropriate model is known as _latent class analysis_ (LCA) or _binomial (finite) mixture model_.
LCA is also known as _latent Dirichlet allocation_ in the machine learning literature.

As a sidenote, Latent Dirichlet allocation is one of the older methods used for natural language processing tasks (Blei et al., 2003).
By old, here, I mean that it is one of the methods used before the advent of deep learning around 2013.
A typical natural language processing tasks would be to classify documents.
In this setting, LDA can be used to interpret text by finding words with a similar "topic".
For example, the topic can be "education" with words such as "school", "students", "schools" and "teachers" (Blei et al., 2003).
This example also shows one of the main differences between LDA usage in machine learning and LCA usage in science.
Both are essentially the same models, but where social science often sticks to a few latent classes to manually interpret, machine learning happily runs the model for 100 latent classes (Blei et al., 2003; p. 1007).

## First attempt

Enough background, let's fit a latent profile model via Turing.
Based on the [Turing tutorial](https://turing.ml/dev/tutorials/01-gaussian-mixture-model/) and a great [Discourse post](https://discourse.julialang.org/t/variational-inference-of-mixture-models/40031/2), we can fit a nice model which runs in less than a minute:

```julia
@model function exchangeable_mixture(k::Int, Y)
    w ~ Dirichlet(k, 1)

    μ ~ filldist(Normal(25, 4), 2)

    n = length(Y)
    Y ~ filldist(MixtureModel(Normal, μ, w), n)
end;
```

```julia
exchangeable_model = exchangeable_mixture(2, combined);
```

```julia
n_samples = 1_000;
```

Just to be sure, let's sample from the prior to see how things look:

```julia
exchangeable_prior = sample(exchangeable_model, Prior(), n_samples);
```

![Exchangeable prior](/files/edf263e7e16b7a98)

This looks as expected.
When looking at the observed distribution in the first figure in this blog, it is not unreasonable to set the means for the `μ` priors around the middle of the data.
With that, the sampler has a nice place to start and should be able to estimate the parameters reasonably quickly.

The `w` specifies the weights of the latent classes.
In the `combined` data, I've drawn half of the samples from `d1` and half of the samples from `d2`, so the weights should be 0.5 and 0.5.
This Dirichlet prior is a, so called, unit simplex.
To me it just looks like an uniform prior, but I guess there is a good reason for the unit simplex term.
This prior is reasonable because it doesn't tell the sampler much about the location of the weights except that they are between 0 and 1.

So, the prior look good.
It's time to obtain the posterior.
To do this, we need to use HMC and not NUTS.
Normally, NUTS is the best sampler to use, but for latent models NUTS is often having problems.
Online, people seem to suggest that this is due to the multimodality, that is, because there are two solutions to this problem:

1. μ1 estimates the mean of Sheperds and μ2 estimates the mean of Terriers **OR**
2. μ2 estimates the mean of Teriers and μ2 estimates the mean of Shepers.

This is called the _identifiability_ problem (Casella & Berger, 2002), the _label switching problem_ (Obserki, 2016) or _labeling degeneracies_ (Betancourt, 2017).

So, let's sample 3 chains in parallel with `HMC`:

```julia
exchangable_posterior = let
    rng = StableRNG(1)
    sampler = HMC(0.2, 10)
    sample(rng, exchangeable_model, sampler, MCMCThreads(), n_samples, 3)
end;
```

![Exchangeable posterior](/files/3276269553b68153)

Hmm.
That didn't work.
When one or more chains don't move at all (a horizontal line in the left plot) for mixture models, then try reducing the leapfrog step size (the first argument to `HMC`).

```julia
exchangable_posterior_smaller_stepsize = let
    rng = StableRNG(1)
    sampler = HMC(0.01, 10)
    sample(rng, exchangeable_model, sampler, MCMCThreads(), n_samples, 4)
end;
```

![Exchangeable posterior](/files/0ab0e523453d0eff)

That looks much better.
However, now we're dealing with the label switching problem.
Normally, to get the parameter estimate, we could just take the mean over all the chains.
In this case, we couldn't do that and instead should take the mean over only one chain?
That would work, but isn't ideal either.

## Fixing the label switching

Betancourt (2017) suggests using an ordered prior for μ.
Via `Bijectors.jl.OrderedBijector` this should be possible in `Turing.jl` too.
Unfortunately, I wasn't able to figure it out.
(It appears that the Stan model is transforming things to the log scale and that works well together with the ordered prior.
I'm too lazy to convert things to the log scale and back again, so that's why I'm not doing that.)

As a workaround, I came up with the idea to enforce the ordering in another way, namely to cutoff the range of possible values via two non-overlapping uniform distributions.
This can be thought of as drawing a line through the middle of the two means which will lock both parameters in their own region.

```julia
@model function mm(k::Int, Y)
    w ~ Dirichlet(k, 1)

    μ1 = Uniform(10, 25)
    μ2 = Uniform(25, 40)
    μ ~ arraydist([μ1, μ2])

    n = length(Y)
    Y ~ filldist(MixtureModel(Normal, μ, w), n)
end;
```

```julia
mixture_model = mm(2, combined);
```

```julia
mixture_model_prior = sample(mixture_model, Prior(), n_samples);
```

![Mixture model prior](/files/9634dfbf293fab99)

After a bit of fiddling and increasing the number of leapfrog steps to use (the second argument to HMC), this shows chains with nice convergence and mixing:

```julia
mixture_posterior = let
    rng = StableRNG(1)
    sampler = HMC(0.01, 20)
    sample(rng, mixture_model, sampler, MCMCThreads(), n_samples, 3)
end;
```

![Mixture posterior](/files/94d234df3f52d398)

| parameters | mean | std | mcse | ess\_bulk | ess\_tail | rhat | ess\_per\_sec
| --- | --- | --- | --- | --- | --- | --- | ---
| `w[1]` | 0.549693 | 0.0843669 | 0.0116713 | 77.4745 | 34.9351 | 1.07559	| 1.12712
| `w[2]` | 0.450307 | 0.0843669 | 0.0116713 | 77.4745 | 34.9351 | 1.07559	| 1.12712
| `μ[1]` | 22.1324 | 1.19495 | 0.347121 | 24.4974	| 18.9747 | 1.10219 | 0.356393
| `μ[2]` | 28.6765 | 1.36629 | 0.138542 | 148.005	| 30.5804 | 1.08689 | 2.1532

And we now have almost correct estimates for all parameter locations.
The correct values should be a mean of 22 and a mean of 28, which are almost correctly estimated as can be seen in the mean column.

## Variational inference

Some say that _variational inference_ (VI) can deal much better with mixed models than Markov chain Monte Carlo.
(I forgot the reference but read it in some paper while trying to debug models.)
Let's put that claim to the test.

VI doesn't have such a nice interface as the Monte carlo based models, but we can run multithreaded sampling with only a few lines of code.
The outcomes are put in a DataFrame here to allow for easier plotting:

```julia
function sample_vi(model; samples_per_step=10, max_iters=1_000)
    n_chains = 3
    dfs = Vector{DataFrame}(undef, n_chains)
    colnames = names(mixture_model_prior, :parameters)
    Threads.@threads for i in 1:n_chains
        q = vi(model, ADVI(samples_per_step, max_iters))
        M = rand(q, n_samples)::Matrix{Float64}
        df = DataFrame(transpose(M), colnames)
        df[!, :chain] = fill(i, nrow(df))
        df[!, :iteration] = 1:nrow(df)
        dfs[i] = df
    end
    vcat(dfs...)
end;
```

```julia
vi_posterior = sample_vi(mixture_model);
```

![VI posterior](/files/3885567395ad8e30)

parameters | mean | std
--- | --- | ---
`w[1]` | 0.536976 | 0.0339081
`w[2]` | 0.463024 | 0.0339081
`μ[1]` | 21.6626 | 0.107995
`μ[2]` | 28.6174 | 0.115107

This outcome is

- closer to the correct outcome than some of the Monte Carlo based posteriors,
- is easier to get right (less fiddling with sampler parameters) and
- runs about 4 times as fast at the time of writing (about 20 seconds for Monte Carlo samplers above versus 5 seconds for VI).

**One caveat though: don't run only one VI chain on the exchangeable model above!**
It will happily give a completely incorrect outcome without showing sampling problems!
To avoid that, run multiple VI chains like shown here.

## Ordered constraint

The drawback of the earlier solutions to the label switching problem is that an estimate should be available for the location of the distributions.
This isn't always the case, especially when the problem would involve more latent distributions than presented here.
A better solution would be to enforce an ordering on the means μ, that is, to enforce that μ1 ≤ μ2 ≤ ... ≤ μn for n means.
With such an ordering, it is impossible for the labels to switch.

After lots and lots of fiddling, I did manage to use an ordered prior in Turing.jl.
Thanks to help by Tor Fjelde in a [GitHub issue](https://github.com/TuringLang/Bijectors.jl/issues/209).
The trick is to use the `Bijectors.OrderedBijector`, put the desired values through the inverse of the bijector and put these outcomes in an `ordered(arraydist(...))`.

Also, for technical reasons, the numbers put through the inverse cannot be the same, so that's why the second number is slightly larger.
I've fiddled a bit with how much difference there is between the numbers and a smaller difference shows a better prior plot, but worse HMC posterior.
Very strange.

```julia
inv_ordered(X::Vector) = Bijectors.inverse(Bijectors.OrderedBijector())(X);
```

```julia
M = inv_ordered([25, 25.01])
```

```raw
2-element Vector{Float64}:
 25.0
 -4.6051701859879355
```

```julia
@model function ordered_mixture(k::Int, Y)
    w ~ Dirichlet(k, 1)

    μ ~ ordered(arraydist([Normal(m, 4) for m in M]))

    n = length(Y)
    Y ~ filldist(MixtureModel(Normal, μ, w), n)
end;
```

```julia
ordered_model = ordered_mixture(2, combined);
```

In the end, the HMC sampler keeps being the most robust.
I've tried `NUTS(1_000, 20)` too and it did work albeit taking minutes to finish and giving erratic estimates.
Also, I've tried VI and that just didn't work with the ordering constraint and no amount of sampler parameter tuning seemed to solve the problem.

So, there we go, let's see the **best** model for the data:

```julia
ordered_hmc_posterior = let
    rng = StableRNG(1)
    sampler = HMC(0.001, 100)
    sample(rng, ordered_model, sampler, MCMCThreads(), 2 * n_samples, 3)
end;
```

![Ordered HMC posterior](/files/06192957e148f9fa)

One last thing. Convergence takes a while, so let's throw away the first 700 samples.

```julia
ordered_warmed_posterior = let
    rng = StableRNG(1)
    sampler = HMC(0.001, 100)
    discard_initial = 1000
    sample(rng, ordered_model, sampler, MCMCThreads(), 2 * n_samples, 3; discard_initial)
end;
```

![Ordered warmed posterior](/files/16b3103bc11039bc)

| parameters | mean | std | mcse | ess\_bulk | ess\_tail | rhat | ess\_per\_sec |
| --- | --- | --- | --- | --- | --- | --- | --- |
| `w[1]` | 0.535497 | 0.0356642 | 0.00129298 | 760.682 | 1651.15 | 1.00207 | 2.94344 |
| `w[2]` | 0.464503 | 0.0356642 | 0.00129298 | 760.682 | 1651.15 | 1.00207 | 2.94344 |
| `μ[1]` | 21.6445 | 0.109563 | 0.00283203 | 1497.22 | 2632.79 | 1.00105 | 5.79344 |
| `μ[2]` | 28.6451 | 0.112493 | 0.0117881 | 91.0354 | 245.47 | 1.02564 | 0.352259 |

Awesome.

## Conclusion

It is very tricky to get mixture modeling right with Markov chain Monte Carlo.
When sampling from the posterior, the sampler couldn't deal well with the label switching problem.
Fiddling like this with Bayesian samplers has benefits too, namely that the problematic sampling did indicate a problem in the model specification.

There were two solutions:
One solution is to define very strong location priors so that `μ[1]` and `μ[2]` have difficulty switching places.
This works with VI and, in turn, reduces the running time and is less sensitive to the choice of sampler parameters.
The drawback is that much information is required to know where the location should be or what is a good cutoff point.
Another solution is to use the ordered constraint.
Unfortunately, I couldn't get this to work with VI nor with NUTS.
Therefore, some manual tuning of sampler parameters is required to get a good outcome.
The running time is reasonable with about 20 seconds for the last model in this post.

Overall, it took about six days to write this blog which is a bit more than I would have guessed.
The main reason why it took so long was that Turing.jl provides a lot of flexibility when defining the models.
In other words, there are many ways in which you can shoot yourself in the foot.
At the same time, it's really great to know that tuning models to specific use-cases is possible unlike frequentist models.
Hopefully, this post will provide a helpful foundation for more complex Bayesian mixed models.

## References

Betancourt, M.
Identifying Bayesian Mixture Models.
Stan documentation.
<https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html>.

Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).
Latent dirichlet allocation.
the Journal of machine Learning research, 3, 993-1022.
<http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>.

Casella, G. & Berger, R. L. (2002).
Statistical Inference.
Second edition.
Cengage learning.

Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).
Bayesian data analysis.
Chapman and Hall/CRC.
<https://doi.org/10.1201/9780429258411>.

Oberski D. L. (2016)
Mixture Models: Latent Profile and Latent Class Analysis.
In: Robertson J., Kaptein M. (eds) Modern Statistical Methods for HCI.
Human–Computer Interaction Series. Springer, Cham.
<https://doi.org/10.1007/978-3-319-26633-6_12>.

## Appendix

```julia
function plot_latent(d1, d2, combined)
	w, h = 1100, 500
	fig = Figure(; size=(w, h))

	ax1 = Axis(fig[1, 1]; title="Observed distribution", xlabel="Weight (kg)")
	density!(ax1, combined)

	title = "Latent distributions"
	ax2 = Axis(fig[1, 2]; title, xlabel="Weight (kg)")
	lower = quantile(d1, 0.001)
	upper = quantile(d2, 0.999)
	I = lower:0.01:upper
	lines!(ax2, I, pdf.(d1, I); label="Australian\nShepherd")
	lines!(ax2, I, pdf.(d2, I); label="American\nTerrier")
	Legend(fig[1, 3], ax2)
	linkxaxes!(ax1, ax2)

	fig
end;
```

```julia
function plot_chains(chns; density_func=density!)
	df = DataFrame(chns)
	n_chains = length(unique(df.chain))
	n_samples = nrow(df) / n_chains
	df[!, :chain] = string.(df.chain)
	coefs = select(df, :iteration, :chain, r"μ*", r"w*")
	cols = filter(n -> startswith(n, r"μ|w") || n == "σ", names(coefs))

	size = (900, 1200)
	fig = Figure(; size)

	values_axs = [Axis(fig[i, 1]; ylabel=string(c)) for (i, c) in enumerate(cols)]
	for (ax, col) in zip(values_axs, cols)
		for i in 1:n_chains
			chain = string(i)
			values = filter(:chain => ==(chain), df)[:, col]
			lines!(ax, 1:n_samples, values; label=chain)
		end
	end
	values_axs[end].xlabel = "Iteration"

	density_axs = [Axis(fig[i, 2]; ylabel=string(c)) for (i, c) in enumerate(cols)]
	for (ax, col) in zip(density_axs, cols)
		for i in 1:n_chains
			chain = string(i)
			values = filter(:chain => ==(chain), df)[:, col]
			density_func(ax, values; label=chain)
		end
	end
	density_axs[end].xlabel = "Parameter estimate"
	w_axs = filter(ax -> startswith(ax.ylabel.val, "w"), density_axs)
	linkxaxes!(w_axs...)
	μ_axs = filter(ax -> startswith(ax.ylabel.val, "μ"), density_axs)
	linkxaxes!(μ_axs...)

	return fig
end;
```

Built with Julia 1.11.3 and

Bijectors 0.14.2 \
CairoMakie 0.12.16 \
DataFrames 1.7.0 \
StableRNGs 1.0.2 \
Statistics 1.11.1 \
Turing 0.35.2

