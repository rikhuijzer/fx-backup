---
created: '2025-05-06 14:41:03 UTC'
updated: '2025-05-06 14:42:34 UTC'
---

# Collinear Bayes

In my post on [Shapley values and multicollinearity](/posts/39), I looked into what happens when you fit a complex uninterpretable model on collinear or _near_-collinear data and try to figure out which features (variables) are important.
The results were reasonable but not great.
Luckily, there are still more things to try.
Gelman et al. ([2020](https://doi.org/10.1017/9781139161879)) say that Bayesian models can do reasonably well on collinear data because they show high uncertainty in the estimated coefficients.
Also, Bayesian models have a chance of fitting the data better as is beautifully shown in the [Stan documentation](https://mc-stan.org/users/documentation/case-studies/golf.html).
It can be quite tricky to implement though because a good parameterization is necessary (<https://statmodeling.stat.columbia.edu/2019/07/07/collinearity-in-bayesian-models/>).

## Simulating data

Let's simulate some data with various columns are increasingly correlated with the outcome (and thus each other).
Here, we assume that the data is centered around zero.
This is easier for the Bayesian model to work with, but can often also make interpretation of the coefficients easier.
There are various methods to rescale data, one is using `MLDataUtils: rescale!`.
Note that `rescale!` bases the rescaling on the sample which is not recommended for small samples (Gelman, [2020](https://doi.org/10.1017/9781139161879)).
Instead, you can use knowledge that you have about the data such as the range of questionnaire scores or the weight of cars.
Specifically, for example, it could be known for the data that the weight of a car is never below zero and unlikely to be above 3_600 kg (8_000 lbs); the weight of a Hummer H1.

```julia
using CairoMakie
using CategoricalArrays: categorical
using DataFrames: Not, DataFrame, select, stack, transform
using GLM
using Turing
using Random: seed!
using Statistics: rand, mean, cor
```

```julia
indexes = 1.0:150.0;
```

```julia
y_true(x) = x / last(indexes);
```

```julia
y_noise(x, corr_coefficient) = (corr_coefficient * y_true(x) - 0.5) + rand(Normal(0, 0.15));
```

```julia
df = let
    seed!(0)
    X = indexes
    A = y_noise.(indexes, 0)
    B = y_noise.(indexes, 0.05)
    C = y_noise.(indexes, 0.7)
    D = y_noise.(indexes, 1)
    E = y_noise.(indexes, 1)
    Y = y_noise.(indexes, 1)

    DataFrame(; X, A, B, C, D, E, Y)
end
```

X | A | B | C | D | E | Y
--- | --- | --- | --- | --- | --- | ---
1.0 | -0.358554 | -0.522957 | -0.481998 | -0.492793 | -0.466424 | -0.552454
2.0 | -0.479912 | -0.748431 | -0.445867 | -0.437615 | -0.636698	| -0.931691
3.0 | -0.27124 | -0.5285 | -0.524026 | -0.45503	| -0.810672 | -0.369753
4.0 | -0.481415 | -0.50173 | -0.783662 | -0.437662 | -0.131537 | -0.0454284
5.0 | -0.680866 | -0.488108 | -0.338793 | -0.420014 | -0.646305 | -0.390959
6.0 | -0.453227 | -0.469888 | -0.300942 | -0.541693 | -0.352914 | -0.672081
7.0 | -0.535196 | -0.498834 | -0.543361	| -0.405727 | -0.385467 | -0.324522
8.0 | -0.663103 | -0.429192 | -0.428918	| -0.22254 | -0.328334 | -0.524071
9.0 | -0.430653 | -0.819506 | -0.398711	| -0.344799 | -0.506967 | -0.501721
10.0 | -0.512089 | -0.614823 | -0.574396 | -0.678601 | -0.541517 | -0.357891
... | ... | ... | ... | ... | ... | ...
150.0 | -0.567046 | -0.421028 | 0.288692 | 0.50339 | 0.19394 | 0.328947

