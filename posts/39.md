---
created: '2025-05-06 09:37:11 UTC'
updated: '2025-05-06 10:01:30 UTC'
---

# Random forest, Shapley values and multicollinearity

Linear statistical models are great for many use-cases since they are easy to use and easy to interpret.
Specifically, linear models can use _features_ (also known as _independent variables_, _predictors_ or _covariates_) to predict an _outcome_ (also known as _dependent variables_).

In a linear model, a higher coefficient for a feature, the more a feature played a role in making a prediction.
However, when variables in a regression model are correlated, these conclusions don't hold anymore.

One way to solve this is to use clustering techniques such as principal component analysis (PCA) (Dormann et al., [2012](https://doi.org/10.1111/j.1600-0587.2012.07348.x)).
With PCA, latent clusters are automatically determined.
Unfortunately, these latent clusters now became, what I would like to call, magic blobs.
Proponents of these techniques could say:
"But we know that there **is** an underlying variable which **causes** our effect, why can't this variable be the same as the cluster that we found?"
Well, because these blobs are found in the data and not in the real-world.

To link these blobs (officially, clusters) back to the real-world, one can try to find the features closes to the blobs in one way or another, but this will always introduce bias.
Another approach is to drop features which are highly correlated and expected to be less important.

Some say that random forests combined with Shapley values can deal with collinearity reasonably well.
This is because the random forest can find complex relations in the data and because Shapley values are based on mathematically proven ideas.
Others say that the Shapley values will pick one of the correlated features and ignore the others.
In this post, I aim to simulating collinear data and see how good the conclusions of the model are.

## Simulating data

```julia
using CairoMakie
using DataFrames: Not, DataFrame, select
using Distributions: Normal
using LightGBM.MLJInterface: LGBMRegressor
using MLJ: fit!, machine, predict
using Random: seed!
using Shapley: MonteCarlo, shapley
using StableRNGs: StableRNG
using Statistics: cor, mean
```

```julia
y_true(x) = 2x + 10;
```

```julia
y_noise(x, coefficient) = coefficient * y_true(x) + rand(Normal(0, 40));
```

```julia
indexes = 1.0:150.0;
```

```julia
r2(x) = round(x; digits=2);
```

```julia
df = let
    seed!(0)
    X = indexes
    T = y_noise.(indexes, 0)
    U = y_noise.(indexes, 0.05)
    V = y_noise.(indexes, 0.7)
    W = y_noise.(indexes, 1)
    Y = y_noise.(indexes, 1)

    DataFrame(; X, T, U, V, W, Y)
end
```

X | T | U | V | W | Y
--- | --- | --- | --- | --- | ---
1.0 | 37.7188 | -5.61088 | 11.9561 | 12.1441 | 19.1759
2.0 | 5.35691 | -65.7259 | 21.7467 | 27.0804 | -26.0083
3.0 | 61.0028 | -7.06661 | 1.05973 | 22.6588 | -72.1792
4.0 | 4.95605 | 0.0830817 | -68.0209 | 27.5124 | 109.146
5.0 | -48.2309 | 3.72674 | 50.7662 | 32.4407 | -27.9034
6.0 | 12.4727 | 8.59667	| 61.0154 | 0.215236 | 50.5563
7.0 | -9.38565 | 0.888579 | -3.47392 | 36.6952 | 42.0977
8.0 | -43.4941 | 19.4709 | 27.1996 | 85.7672 | 57.5553
9.0 | 18.4924 | -84.6015 | 35.4103 | 53.3868 | 10.1422
10.0 | -3.22372 | -30.0084 | -11.2833 | -35.4048 | 1.15114
... | ... | ... | ... | ... | ...
150 | 150.0 | -17.879 | 23.2258 | 240.651 | 310.904 | 228.384

![data](/files/e80f59ace2862d34)

