---
created: '2025-05-05 12:21:34 UTC'
updated: '2025-05-05 13:02:36 UTC'
---

# Random forest classification in Julia

Below is example code for fitting and evaluating a linear regression and random forest classifier in Julia.
I've added the linear regression as a baseline for the random forest.
The models are evaluated on a mock variable $U$ generated from two distributions, namely

$$
\begin{aligned}
d_1 &= \text{Normal}(10, 2) \: \: \text{and} \\
d_2 &= \text{Normal}(12, 2),
\end{aligned}
$$

The random variable $V$ is just noise meant to test the classifier, generated via

$$
V \sim \text{Normal}(100, 10)
$$

This data isn't meant to show that random forests are good classifiers per se.
It is just meant to show how to fit and plot random forests in Julia.
One way to show that random forests are accurate would be to have about the same or more variables than observations (Biau & Scornet, [2016](https://doi.org/10.1007/s11749-016-0481-7)).

## Data generation

Let's load some packages and generate the data:

```julia
import MLJGLMInterface
import MLJDecisionTreeInterface

using CairoMakie
using CategoricalArrays
using Colors: RGB
using DataFrames
using Distributions
using MLJBase
using MLJ
using StableRNGs: StableRNGs
using Random
```

```julia
classlabels = ["A", "B"];
```

```julia
df = let
    # Number of elements per class.
    n = 70
    μ1 = 10
    μ2 = 12
    σ = 2

    d1 = Normal(μ1, σ)
    d2 = Normal(μ2, σ)

    Random.seed!(12)
    classes = repeat(classlabels, n)

    df = DataFrame(
        class = categorical(classes),
        U = [class == "A" ? rand(d1) : rand(d2) for class in classes],
        V = rand(Normal(100, 10), 2n)
    )
end
```

class | U | V
--- | --- | ---
"A" | 10.6777 | 98.1947
"B" | 14.0715 | 101.393
"A" | 7.0505 | 119.386
"B" | 17.3497 | 89.6641
"A" | 8.29689 | 97.1358
"B" | 11.2469 | 94.0527
"A" | 9.91297 | 99.3432
"B" | 15.6322 | 109.662
"A" | 8.9616 | 105.435
"B" | 13.6919 | 110.949
... | ... | ...
"B" | 13.22 | 93.4114

```julia
X = (; df.U, df.V);
```

```julia
y = df.class;
```

If we plot this, we can see that the points of `"B"` lie more to the right.
More specifically, the points for `"B"` are higher on average for `U`:

```julia
let
    fig = Figure()
    ax = Axis(fig[1, 1]; xlabel="U", ylabel="V")
    classmarkers = [:xcross, :circle]
    for (label, marker) in zip(classlabels, classmarkers)
        filtered = filter(:class => ==(label), df)
        scatter!(ax, filtered.U, filtered.V; label, marker)
    end
    Legend(fig[1, 2], ax, "class")
    fig
end
```

![data](/files/f7d64ec05f921d14)

## Train and test split

Let's split our data before continuing.
Training and evaluating (testing) on the same data is not great because we want to know how well our model generalizes.
It is easy to make correct predictions when you have seen the data which you need to predict already.
For more information, see topics such as [overfitting](https://en.wikipedia.org/wiki/Overfitting).
So, to avoid this problem, we split the data up in a train and test set.

```julia
train, test = let
    rng = StableRNG(123)
    MLJ.partition(eachindex(df.class), 0.7; shuffle=true, rng)
end;
```

## Model fitting

Now, we can fit a model in order to determine the accuracy later:

```julia
logistic = let
    LinearBinary = @load LinearBinaryClassifier pkg=GLM verbosity=0
    LinearBinary()
end;
```

```julia
fitted_logistic = let
    Random.seed!(11)
    mach = machine(logistic, X, y)
    fit!(mach; rows=train)
    mach
end;
```

```julia
r2(x) = round(x; digits=2);
```

```julia
coefficients = r2.(fitted_params(fitted_logistic).coef)
```

```raw
2-element Vector{Float64}:
 0.44
 0.03
```

